{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5412037,"sourceType":"datasetVersion","datasetId":3132565}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\nfrom transformers import pipeline, set_seed\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install -q -U bitsandbytes\n!pip3 install -q -U peft==0.8.2\n!pip3 install -q -U trl==0.7.10\n!pip3 install -q -U accelerate==0.27.1\n!pip3 install -q -U datasets==2.17.0\n!pip3 install -q -U transformers==4.38.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"!pip3 install -q -U datasets==2.17.0","metadata":{}},{"cell_type":"code","source":"import os\nimport transformers\nimport torch\nfrom google.colab import userdata\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nfrom peft import LoraConfig\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers import BitsAndBytesConfig","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summarizer = pipeline(\"summarization\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/farmers-call-query-data-qa/questionsv4.csv', delimiter=',')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data cleaning","metadata":{}},{"cell_type":"code","source":"#change character lower case\ndata['questions']=data['questions'].str.lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#drop NULL\ndata.dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data =data.drop_duplicates()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tokenization","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nstring.punctuation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_punct(text):\n    translator = str.maketrans('', '', string.punctuation)\n    result=text.translate(translator)\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['questions']=data.questions.apply(remove_punct)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove Stop Words","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\nstop=set(stopwords.words('english'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_stopword(text):\n    word_tokens = word_tokenize(text)\n    result = [w for w in word_tokens if not w.lower() in stop]\n    return ' '.join(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['questions']=data.questions.apply(remove_stopword)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.questions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lemmetizing","metadata":{}},{"cell_type":"code","source":"# nltk.download()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport subprocess\n\n# Download and unzip wordnet\ntry:\n    nltk.data.find('wordnet.zip')\nexcept:\n    nltk.download('wordnet', download_dir='/kaggle/working/')\n    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n    subprocess.run(command.split())\n    nltk.data.path.append('/kaggle/working/')\n\n# Now you can import the NLTK resources as usual\nfrom nltk.corpus import wordnet","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer=WordNetLemmatizer()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemmatizer.lemmatize(\"kids\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lemm(text):\n    list2 = nltk.word_tokenize(text)\n    return ' '.join([lemmatizer.lemmatize(words) for words in list2])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.questions=data.questions.apply(lemm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save and load dataframe as a pkl file","metadata":{}},{"cell_type":"code","source":"# data.to_pickle(\"data.pkl\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = pd.read_pickle(\"/kaggle/working/data.pkl\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Vectoriser","metadata":{}},{"cell_type":"code","source":"corpus=data['questions'].values\ncorpus","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bag of words","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nbw_vect = CountVectorizer()\n# tokenize et construire le vocabulaire\nbw_fit=bw_vect.fit(corpus)\n# vectoriser les mots\nbw_corpus = bw_fit.transform(corpus)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bw_corpus.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bw_fit.get_feature_names_out()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bw_corpus.toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_data=pd.DataFrame(bw_corpus.toarray(),columns=bw_fit.get_feature_names_out())\ncv_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TF-IDF\n\nThis is a statistical measure used to assess the importance of a term within a document, relative to a collection or corpus. The weight increases proportionally with the number of occurrences of the word in the document. It also varies based on the frequency of the word in the corpus.\n\n- TF : Term Frequency describes how often a certain term appears in a document compared to all other terms in the document.\n$$TF(m,p)=\\frac{f_{m,p}}{f_p}$$\n$f_{m,p}$: frequency of the word $m$ in the sentence $p$, $f_p$  number of words in the sentence  $p$\n\n- IDF : IDF measures the significance of a term not in relation to its frequency in a particular document, but in relation to its distribution and usage across all documents.\n$$IDF(m)=\\log (\\frac{L}{L_m})$$\n$L$ : number of sentences in the corpus, $L_m$ :  number of sentences in the corpus where the word $m$ appears\n\n- TF-IDF : This is the multiplication of the two values. Since Term Frequency represents the relevance of a term in a given document and Inverse Document Frequency can reflect the role of a term in relation to all the documents in a corpus, the combination of the two values helps in understanding the actual frequency of terms and the potential of each term.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_vect = TfidfVectorizer(max_features=5000)\n\ntfidf_fit=tf_vect.fit(corpus)\n\ntfidf_corpus= tfidf_fit.transform(corpus)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load and Save the vectorizer as pkl","metadata":{}},{"cell_type":"code","source":"corpus = np.array([\"aaa bbb ccc\", \"aaa bbb ddd\"])\nvectorizer = CountVectorizer(decode_error=\"replace\")\nvec_train = vectorizer.fit_transform(corpus)\n#Save vectorizer.vocabulary_\npickle.dump(vectorizer.vocabulary_,open(\"feature.pkl\",\"wb\"))\n\n#Load it later\ntransformer = TfidfTransformer()\nloaded_vec = CountVectorizer(decode_error=\"replace\",vocabulary=pickle.load(open(\"feature.pkl\", \"rb\")))\ntfidf = transformer.fit_transform(loaded_vec.fit_transform(np.array([\"aaa ccc eee\"])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_fit.get_feature_names_out()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_data=pd.DataFrame(tfidf_corpus.toarray(),columns=tfidf_fit.get_feature_names_out())\ntfidf_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test=' how to avail kisan credit card loan for sali crop.'\ntfidf_test=tfidf_fit.transform([test])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask=tfidf_test.toarray()!=0\nm=mask[0]\nm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_test.toarray()[mask]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_data.columns[m]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\ncm=cosine_similarity(tfidf_test, tfidf_corpus)\ncm[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\npos=np.argmax(cm[0])\ndata.iloc[pos]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.answers[pos]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Function response\n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport nltk\nimport pandas as pd\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport re\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport random","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemm = WordNetLemmatizer()\n\ndef clean_data(corpus):\n    result = []\n    for sentence in corpus:\n        # Removing parenthesis from sentence\n        sentence = re.sub(r\"[\\([{})\\]]\",\" \" , sentence)\n        # Convert to list of words\n        sentence = sentence.split()\n        # Lemmatize each word\n        sentence = [lemm.lemmatize(word) for word in sentence]\n        # Form string from list and append to result\n        result.append(\" \".join(sentence))\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Summarizer","metadata":{}},{"cell_type":"code","source":"text = \"\"\"What is the optimal planting time for tomatoes in our region, considering the current weather conditions and soil moisture levels? Additionally, which tomato varieties have shown the best adaptability to our specific microclimate and soil composition in recent years?\"\"\"\nsummary=summarizer(text, max_length=40, min_length=10, do_sample=False)[0]\nprint(\"\\n\",summary['summary_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def summarize_input(test):\n    summary=summarizer(text, max_length=20, min_length=10, do_sample=False)[0]\n    return summary['summary_text']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testtt = \"What is the optimal planting time for tomatoes in our region, considering the current weather conditions and soil moisture levels? Additionally, which tomato varieties have shown the best adaptability to our specific microclimate and soil composition in recent years?\"\nif len(nltk.word_tokenize(testtt)) >2:\n    print(summarize_input(testtt))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tf_vect = TfidfVectorizer(max_features=5000)\n\n# tfidf_fit=tf_vect.fit(corpus)\n\n# tfidf_corpus= tfidf_fit.transform(corpus)\n\nwelcome = ['hi', 'hey']\n\ndef chatbot_response(test):\n    # If user inputs any of the greeting words, give greeting in response\n    for w in test.split():\n        if w.lower() in welcome:\n            return random.choice(welcome)\n    if len(nltk.word_tokenize(test)) >20:\n        test=summarize_input(test)\n    tfidf_test=tfidf_fit.transform([test])\n    mask=tfidf_test.toarray()!=0\n    m=mask[0]\n    tfidf_test.toarray()[mask]\n    cm=cosine_similarity(tfidf_test, tfidf_corpus)\n    pos=np.argmax(cm[0])\n    data.iloc[pos]\n    return data.answers[pos]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Chatbot","metadata":{}},{"cell_type":"code","source":"# # Remove comments to run chatbot\n# print(\"FARM BOT: Hi!! Type bye to exit. Ask me anything: \")\n# while(True):\n#     user_chat = input()\n#     if(user_chat.lower()==\"bye\"):\n#         print(\"Bye\")\n#         break\n#     print(\"Farm BOT: \", end=\" \")\n#     print(chatbot_response(user_chat))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Eng-Hindi chat","metadata":{}},{"cell_type":"code","source":"from transformers import MBartForConditionalGeneration, MBart50TokenizerFast","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = MBartForConditionalGeneration.from_pretrained(\"SnypzZz/Llama2-13b-Language-translate\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = MBart50TokenizerFast.from_pretrained(\"SnypzZz/Llama2-13b-Language-translate\", src_lang=\"en_XX\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"article_en = \"FARM BOT: Hi!! Type bye to exit. Ask me anything\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_inputs = tokenizer(article_en, return_tensors=\"pt\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# translate from English to Hindi\ngenerated_tokens = model.generate(\n    **model_inputs,\n    forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"]\n)\n\n# => 'संयुक्त राष्ट्र के नेता कहते हैं कि सीरिया में कोई सैन्य समाधान नहीं है'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def toHindi(en):\n    model_inputs = tokenizer(en, return_tensors=\"pt\")\n    generated_tokens = model.generate(**model_inputs,forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"])\n    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"welcome = ['hi', 'hey']\n\ndef chatbot_response_hindi(test):\n    # If user inputs any of the greeting words, give greeting in response\n    for w in test.split():\n        if w.lower() in welcome:\n            return random.choice(welcome)\n    if len(nltk.word_tokenize(test)) >20:\n        test=summarize_input(test)\n    tfidf_test=tfidf_fit.transform([test])\n    mask=tfidf_test.toarray()!=0\n    m=mask[0]\n    tfidf_test.toarray()[mask]\n    cm=cosine_similarity(tfidf_test, tfidf_corpus)\n    pos=np.argmax(cm[0])\n    data.iloc[pos]\n    return toHindi(data.answers[pos])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Remove comments to run chatbot\n# print(\"FARM BOT: हाय!! बाहर निकलने के लिए बाई टाइप करें. मुझे कुछ भी पूछें: \")\n# while(True):\n#     user_chat = input()\n#     if(user_chat.lower()==\"bye\"):\n#         print(\"Bye\")\n#         break\n#     print(\"Farm BOT: \", end=\" \")\n#     print(chatbot_response_hindi(user_chat))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hin-Hin Chat","metadata":{}},{"cell_type":"code","source":"from transformers import MBartForConditionalGeneration, MBart50TokenizerFast","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Quantized model(my modification)**","metadata":{}},{"cell_type":"code","source":"bnb_config= BitsAndBytesConfig(\n  load_in_4bit=True,\n  bnb_4bit_quant_type=\"nf4\",\n  bnb_4bit_compute_dtype=torch.bfloat16\n  )\n\ntokenizer2 = AutoTokenizer.from_pretrained.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\nmodel2 = AutoModelForCausalLM.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\",quantization_config=bnb_config,\n                                             device_map={\"\":0})\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer2.src_lang = \"hi_IN\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def toEnglish(en):\n    encoded_hi = tokenizer2(en, return_tensors=\"pt\")\n    generated_tokens = model2.generate(**encoded_hi,forced_bos_token_id=tokenizer2.lang_code_to_id[\"en_XX\"])\n    return tokenizer2.batch_decode(generated_tokens, skip_special_tokens=True)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"welcome = ['hi', 'hey']\n\ndef chatbot_response_hindiFull(test):\n    # If user inputs any of the greeting words, give greeting in response\n    test = toEnglish(test)\n    if len(nltk.word_tokenize(test)) >20:\n        test=summarize_input(test)\n    tfidf_test=tfidf_fit.transform([test])\n    mask=tfidf_test.toarray()!=0\n    m=mask[0]\n    tfidf_test.toarray()[mask]\n    cm=cosine_similarity(tfidf_test, tfidf_corpus)\n    pos=np.argmax(cm[0])\n    data.iloc[pos]\n    return toHindi(data.answers[pos])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Remove comments to run chatbot\n# print(\"FARM BOT: हाय!! बाहर निकलने के लिए बाई टाइप करें. मुझे कुछ भी पूछें: \")\n# while(True):\n#     user_chat = input()\n#     if(user_chat.lower()==\"bye\"):\n#         print(\"Bye\")\n#         break\n#     print(\"Farm BOT: \", end=\" \")\n#     print(chatbot_response_hindiFull(user_chat))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}